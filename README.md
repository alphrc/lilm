[‰∏≠Êñá](./README_ZH.md) ÔΩú English

<p align="center"><img src="assets/icon.jpg" width="150"/></p>

<p align="center">
Originality:
<img src="assets/star_yellow.png" width="15"/>
<img src="assets/star_yellow.png" width="15"/>
<img src="assets/star_yellow.png" width="15"/>
<img src="assets/star_blank.png" width="15"/>
<img src="assets/star_blank.png" width="15"/>
&nbsp
Innovation:
<img src="assets/star_yellow.png" width="15"/>
<img src="assets/star_yellow.png" width="15"/>
<img src="assets/star_blank.png" width="15"/>
<img src="assets/star_blank.png" width="15"/>
<img src="assets/star_blank.png" width="15"/>
&nbsp
Challenge:
<img src="assets/star_yellow.png" width="15"/>
<img src="assets/star_yellow.png" width="15"/>
<img src="assets/star_blank.png" width="15"/>
<img src="assets/star_blank.png" width="15"/>
<img src="assets/star_blank.png" width="15"/>
</p>

<p align="center">
üõ†Ô∏è <a href="#operation-principles">Operation Principles</a>
ÔΩú
üìÅ <a href="#file-structure">File Structure</a>
ÔΩú
üñ•Ô∏è <a href="#usage-instructions">Usage Instructions</a>
ÔΩú
üëÄ <a href="#example-results">Example Results</a>
</p>
<p align="center">
üì£ <a href="#common-errors">Common Errors</a>
ÔΩú
üôãüèª‚Äç‚ôÇÔ∏è <a href="#frequently-asked-questions">Frequently Asked Questions</a>
</p>

# LIHKG Language Model (LiLM)

Inspired by [Yi Lin](https://www.youtube.com/@lyi)'s [bilibot project](https://github.com/linyiLYi/bilibot/tree/main) and [video](https://www.youtube.com/watch?v=52clfKcM4M4&t=1s), this experimental project uses responses from users of the [LIHKG forum](https://lihkg.com) with a unique linguistic style for fine-tuning training, creating this Cantonese post response generation language model.

After balancing computing costs and the [Chinese capability of base models](https://github.com/jeinlee1991/chinese-llm-benchmark), the open-source base model selected for this experimental project is [Qwen/Qwen1.5-32B-Chat](https://huggingface.co/Qwen/Qwen1.5-32B-Chat), which has 32 billion parameters. It utilizes the AI-specific framework [MLX](https://github.com/ml-explore/mlx) on the Apple Silicon platform and the [MLX-LM LoRA fine-tuning example](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#fine-tune), leveraging the [LoRA algorithm](https://arxiv.org/abs/2106.09685) on the M3 Max 128GB and M2 Ultra 192GB to fine-tune the base model.

The model has shown significant improvement in Cantonese language ability after fine-tuning, and its tone and style are deeply influenced by the group of [LIHKG](https://zh.wikipedia.org/zh-hk/LIHKGË®éË´ñÂçÄ) users. For more details, see [Example Results](#example-results).

To learn more about artificial intelligence and view more innovative and interesting projects in the future, please follow [alphrc](https://github.com/alphrc).

### Project Motivation
- This project aims to demonstrate the language style imitation capabilities of large language models based on Cantonese spoken data and unique linguistic styles in a forum, primarily used for popular education, academic research, and technical demonstrations, hence the content will be more detailed

### Usage Limitations
- The model training is based on public data, although efforts have been made to clean sensitive content, biases based on training content may still be included, and improper content should be avoided when used
- The generated text reflects specific community culture, understand the relevant background before use
- Conduct sufficient testing before actual application, avoid use in sensitive or controversial situations, and set up monitoring mechanisms to prevent generation of inappropriate content

### Remarks
- All project codes are self-written, and the open-source community members are also encouraged to review the project, provide feedback and suggestions, and directly participate in the improvement of the project
- The nature of this project is the use and practice of third-party training frameworks and models, with main challenges being system configuration, data fetching, data engineering, repeated trial and error, and long waits
- The project has organized some configuration information and content in the `.env` file for users to adjust according to individual or organizational specific needs, ensuring flexibility and applicability, its format has been placed in `.env.template`, and the file name can be changed to `.env` for use


## Operation Principles

### Fine-tuning
Large [pre-trained language model](https://www.kaggle.com/code/vad13irt/language-model-pre-training) possess basic and general human language response capabilities. By [fine-tuning](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)) the model with specific textual data, it can learn further on this data, enhancing its ability to mimic aspects such as tone, style, information, and word usage. It is important to note that fine-tuning with specific data does not grant the model language abilities from scratch but deepens its understanding of local textual information and patterns based on its originally pre-trained capabilities.

### Dataset
This project conducts large-scale public data scraping on the [LIHKG forum](https://lihkg.com) and processes the raw data to create a dataset for fine-tuning. To enhance data quality, the filtering criteria include:

- The first response to the post is not by the author, ensuring the completeness of the information on which the response is based.
- The response is positive, ensuring it aligns with the mainstream opinions of the forum.
- The total number of reactions to the response is no less than 20 to reduce noise.
- It is not a reply to another response.
- It is not the author‚Äôs own response.
- It does not contain any external links or embeds.
- It does not contain sensitive words.
- The total number of words plus system information does not exceed 2048.

These responses, combined with the corresponding post‚Äôs title, content, and category, along with [system message](https://promptmetheus.com/resources/llm-knowledge-base/system-message), are converted into the [format](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#data) required by the MLX-LM LoRA fine-tuning example, and randomly arranged to generate the total dataset. The total dataset is divided into a training set (80%), a validation set (10%), and a testing set (10%), where the testing set's posts have not appeared in the training or validation sets to validate [generalization](https://towardsdatascience.com/generalization-in-ai-systems-79c5b6347f2c) and prevent [overfitting](https://en.wikipedia.org/wiki/Overfitting).

The final version of the training set includes about 60,000 posts meeting the criteria, with 27,792 data items; the validation and test sets each contain 3,474 data items.

### Base Model
The open-source base model [Qwen/Qwen1.5-32B-Chat](https://huggingface.co/Qwen/Qwen1.5-32B-Chat) has 32 billion parameters with a precision of BF16. When the MLX-LM module is run for the first time, if no model is detected in `~/.cache`, it automatically downloads the model from Hugging Face to `~/.cache/huggingface/hub/model--Qwen--Qwen1.5-32B-Chat`. Users do not need to manually pre-download. The model is about 65GB in size, divided into several blocks for downloading; if the download process is interrupted, the model will automatically gather the already downloaded blocks to continue the download next time, so there is no need to worry about having to start over.

### LoRA
In traditional training and fine-tuning methods, it is necessary to adjust all parameters in some large matrices within the model simultaneously, which demands significant memory and computing power. In contrast, [LoRA (Low Rank Adaptation)](https://arxiv.org/abs/2106.09685) uses two smaller matrices to estimate changes in the model's large matrices, significantly reducing the number of parameters. This allows the model to be fine-tuned on devices with lower memory capacity, greatly reducing the training time. In practice, the original total parameter count of the model is 32.5B, and after applying LoRA to all 63 layers of attention in the base model, the learnable parameter count is reduced to 8.3M, only 0.026% of the original.

Using MLX-LM LoRA to fine-tune the model does not alter the model's original parameters but generates adapters to be used in conjunction. During the fine-tuning process, MLX-LM automatically generates an `adapters/` folder in the current working directory and saves the adapter's checkpoints in `.safetensors` format, with each checkpoint about 33.6MB in size. These checkpoints can be used later for further fine-tuning.

### Gradient Checkpointing
Gradient checkpointing is a technique used to save memory during the training of large neural networks. In the neural network training process, effective [backpropagation](https://brilliant.org/wiki/backpropagation/#:~:text=Backpropagation%2C%20short%20for%20%22backward%20propagation,to%20the%20neural%20network's%20weights.) typically requires the retention of intermediate layer outputs for gradient calculation. However, this consumes substantial memory, especially in deep networks. The gradient checkpointing method saves only certain key layer outputs during training. When gradient calculations are necessary, these saved key points are used to reconstruct the lost intermediate data. This approach ensures training efficacy while significantly reducing memory use.

### Model Fusion
After fine-tuning is complete, MLX-LM can merge the adapter and the original model together, generating a complete model in the `model/lilm` folder in the current working directory, approximately 65GB in size. Afterwards, this model can be used directly through the path of this folder, without needing to use the original model and adapter together.


## File Structure
- `src/` : Python code
    - `data.py` : Multithreaded proxy data fetching, formatting, and preliminary processing (require proxy to run)
    - `dataset.py` : Data processing, transformation, and filtering
    - `run.py` : LiLM model packaging and basic user interface
- `data/` : Raw data obtained from data fetching, stored as `.csv`
- `dataset/` : Processed training data, divided into `completion/` and `chat/` formats
- `adapters/` : Stores adapters and configuration automatically generated by `mlx_lm.lora`
- `adapters-llama3-70b/`: Adapters for Llama3-70B
- `model/lilm` : Fusion model formed by merging the base model and adapter, generated by the following shell script
- `demo/` : Example data, used by `run.py`


## Usage Instructions

### Hardware Requirements
This project utilizes the proprietary MLX framework by Apple, hence it can only run on MacOS systems equipped with Apple Silicon Chips (M1 or higher). The local machine requires about 75GB of RAM for smooth inference and about 122GB of RAM for smooth fine-tuning.

### Environment Setup
Run the following shell script to set up and configure the environment using [Anaconda](https://www.anaconda.com) and download all necessary dependencies according to `requirements.txt`.
```bash
conda create -n lilm python=3.9
conda activate lilm
pip install -r requirements.txt
```

### Monitoring System Resource Usage (Optional)
Use the `asitop` module to monitor computer resource usage in real-time through a graphical interface, such as CPU, GPU, and RAM, to ensure the program runs normally.
```bash
sudo asitop
```

### Inference Using the Base Model
The model will automatically download the first time it is run, `--model` can be used for the full name of the model on Hugging Face or its path,
```bash
mlx_lm.generate \
    --model Qwen/Qwen1.5-32B-Chat \
    --prompt "What is LIHKG?"
```

### Fine-tuning
After preparing the `train.jsonl` and `valid.jsonl` datasets in `dataset/chat`, start fine-tuning the model from scratch and generate the `adapters/` folder.
```bash
mlx_lm.lora \
    --model Qwen/Qwen1.5-32B-Chat \
    --train \
    --data dataset/chat \
    --iters 600 \
    --grad-checkpoint
```

### Continue Fine-tuning
Continue fine-tuning using an existing adapter, `--resume-adapter-file` must be a `.safetensors` file.
```bash
mlx_lm.lora \
    --model Qwen/Qwen1.5-32B-Chat \
    --resume-adapter-file adapters/adapters.safetensors \
    --train \
    --data dataset/chat \
    --iters 600 \
    --grad-checkpoint
```
üö® Please note, you are likely to encounter [this error](#error-1).

### Inference with Adapter
Perform generation using the base model combined with an adapter, where the adapter must be a `.safetensors` file.
```bash
mlx_lm.generate \
    --model Qwen/Qwen1.5-32B-Chat \
    --adapter-path adapters/adapters.safetensors \
    --prompt "What is LIHKG?"
```

### Fusion of Base Model and Adapter
The latest checkpoint `adapters.safetensors` in `adapters/` will be automatically selected for fusion, and the fused model will be placed in `model/lilm`.
```bash
mlx_lm.fuse \
    --model Qwen/Qwen1.5-32B-Chat \
    --adapter-path adapters \
    --save-path model/lilm
```

### Inference Using the Fused Model
Use the path of the fused model in `--model`.
```bash
mlx_lm.generate \
    --model model/lilm \
    --prompt "What is LIHKG?"
```

### Model Quantization (Optional)
Use [quantization](https://blog.csdn.net/jinzhuojun/article/details/106955059) to reduce model parameter precision, compress model size, speed up inference, and reduce memory usage. The `--hf-path` is the same as before, can be the full name of the model on Hugging Face, or the model's path, and `--mlx-path` is the path where the compressed model is stored. However, testing shows that quantization significantly decreases model accuracy, and the quantized model cannot run using Hugging Face's Transformer.

```bash
mlx_lm.convert \
    --hf-path model/lilm \
    --mlx-path model/lilm-4Bit \
    -q
```

### Running LiLM
Use `src/run.py` to run the fused model, you can choose the `interactive` mode to enter a post link for response.
```bash
python src/run.py
```


## Example Results

LiLM has shown significant improvement over the base model in Cantonese language ability, and its language style is also influenced by the LIHKG discussion forum. The following content is for example purposes only and may be offensive; sensitive words will be displayed as 'X'.

### Example 1

**Prompt ([Original Post](https://lihkg.com/thread/3699748/page/1)):**
> È°ûÂà•Ôºö„ÄåÂâµÊÑèÂè∞„Äç<br><br>
> Ê®ôÈ°åÔºö„ÄåÊúâÂÜá‰∫∫Ë¶∫ÂæóÂ§ßÂªàÁúãÊõ¥Ê†πÊú¨ÂÜáXÁî®Ôºü„Äç<br><br>
> ÂÖßÂÆπÔºö„ÄåÂîî‰øÇÈòøÂèîÂ∞±‰øÇÈòøÂ¨∏ÔºåÊØ´ÁÑ°ÈòªÂöá‰ΩúÁî®ÔºåÈÅáÂà∞Êúâ‰∫∫ÊâìÂä´ÈÉΩ‰øÇÂåøÈ≥©Âüã„Äç<img src="https://lihkg.com/assets/faces/normal/dead.gif"/>

**LiLM Response:**
> ÊàëÂìãÂë¢Âï≤Â§ßÂªàÈÉΩ‰øÇÂíÅÔºåÊúâÊôÇÁúü‰øÇÊÉ≥ÂïèÔºå‰Ω¢Âìã‰øÇÂí™Áúü‰øÇÁî®‰æÜ‰ΩúÊì∫Ë®≠Ôºü <img style="vertical-align: middle;" src="https://lihkg.com/assets/faces/normal/like.gif" />

or

> ÊúâXÁî®Âí©ÔºüÂ§ßÂªàÁúãÊõ¥‰øÇÂÜáÁî®Êó¢XXÔºåÂÜáÈòªÂêì‰ΩúÁî®ÔºåÂÜáÂæóÊïë <img src="https://lihkg.com/assets/faces/normal/dead.gif"/>

**Base Model Response:**
> <details>
> <summary>Expand Full Content</summary><br>
> Âú®ÂâµÊÑèÂè∞ÁöÑËÉåÊôØ‰∏ãÔºåÈÄôÂÄã‰∏ªÈ°åÂºïËµ∑‰∫ÜË®éË´ñ„ÄÇÊúâ‰∫õ‰∫∫ÂèØËÉΩÊúÉÂêåÊÑè‰Ω†ÁöÑÁúãÊ≥ïÔºåË™çÁÇ∫Â§ßÂªàÁöÑÁúãÊõ¥ÂèØËÉΩ‰∏¶Ê≤íÊúâÊèê‰æõË∂≥Â§†ÁöÑÂÆâÂÖ®ÊÑü„ÄÇ‰ªñÂÄëÂèØËÉΩÊúÉËàâ‰æãÔºåÂÉè‰Ω†ÊèêÂà∞ÁöÑÔºåÁúãÊõ¥ÂèØËÉΩÊòØÂπ¥Èï∑ËÄÖÔºåÂèØËÉΩÂú®Ë∫´È´îÁ¥†Ë≥™ÊàñÂèçÊáâËÉΩÂäõ‰∏äÊúâÊâÄÈôêÂà∂ÔºåÈù¢Â∞çÁ™ÅÁôºÊÉÖÊ≥ÅÊôÇÂèØËÉΩÁÑ°Ê≥ïÊúâÊïàÈòªÊ≠¢ÁäØÁΩ™„ÄÇ<br>
> ÁÑ∂ËÄåÔºå‰πüÊúâ‰∫∫ÊúÉÂº∑Ë™øÁúãÊõ¥ÁöÑÈáçË¶ÅÊÄß„ÄÇ‰ªñÂÄëÂèØËÉΩÊúÉÊåáÂá∫ÔºåÂç≥‰ΩøÁúãÊõ¥‰∏ç‰∏ÄÂÆöËÉΩ physically ÈòªÊ≠¢ÁäØÁΩ™Ôºå‰ªñÂÄëÁöÑÂ≠òÂú®Êú¨Ë∫´Â∞±ÊòØ‰∏ÄÁ®ÆÂ®ÅÊÖëÔºåÂõ†ÁÇ∫‰ªñÂÄëËÉΩÂèäÊôÇÁôºÁèæÁï∞Â∏∏‰∏¶ÈÄöÂ†±„ÄÇÊ≠§Â§ñÔºåÁúãÊõ¥ÈÇÑË≤†Ë≤¨Áõ£Êéß CCTV„ÄÅÁÆ°ÁêÜÂá∫ÂÖ•ÂíåÁ¢∫‰øùÂ§ßÂªàÁöÑÂü∫Êú¨ÂÆâÂÖ®ÔºåÈÄô‰∫õÈÉΩÊòØ‰∏çÂÆπÂøΩË¶ñÁöÑË≤¨‰ªª„ÄÇ<br><br>
> Âè¶Â§ñÔºåÊàëÂÄë‰πüÂèØ‰ª•ÂæûÂè¶‰∏ÄÂÄãËßíÂ∫¶‰æÜÁúãÈÄôÂÄãÂïèÈ°åÔºöÊòØÂê¶ÊáâË©≤ÊúüÂæÖÁúãÊõ¥ÊàêÁÇ∫Â∞àÊ•≠ÁöÑÊ≤ªÂÆâÁ∂≠Ë≠∑ËÄÖÔºüÊàñËÄÖÊàëÂÄëÈúÄË¶ÅÊõ¥ÂÖ®Èù¢ÁöÑÊ≤ªÂÆâÊé™ÊñΩÔºåÂ¶ÇÂä†Âº∑Ë≠¶ÊñπÂ∑°ÈÇè„ÄÅÊèêÂçáÁ§æÂçÄË≠¶Ë¶∫ÊÄßÔºåÁîöËá≥Âà©Áî®ÂÖàÈÄ≤ÊäÄË°ìÂ¶Ç AI Áõ£ÊéßÁ≠âÔºü<br><br>
> Á∏Ω‰πãÔºåÈÄôÂÄãË©±È°åÊ∂âÂèäÂ∞çÂÆâÂÖ®ÈúÄÊ±ÇÁöÑÁêÜËß£„ÄÅÂ∞çÁ§æÂçÄËßíËâ≤ÁöÑÂÆöÁæ©Ôºå‰ª•ÂèäÂ¶Ç‰ΩïÊúÄÊúâÊïàÂú∞ÂàÜÈÖçË≥áÊ∫ê‰ª•‰øùÈöúÂ§ßÂÆ∂ÁöÑÂÆâÂÖ®„ÄÇÊØèÂÄã‰∫∫ÂèØËÉΩÈÉΩÊúâËá™Â∑±Áç®ÁâπÁöÑËßÄÈªûÔºåÈÄôÂ∞±ÊòØÂâµÊÑèÂè∞ÁöÑÈ≠ÖÂäõ‰πã‰∏ÄÔºåËÆìÂ§ßÂÆ∂ÂèØ‰ª•Ëá™Áî±‰∫§ÊµÅ‰∏çÂêåÁöÑÊÉ≥Ê≥ï„ÄÇ
> </details>

**Base Model Response with System Prompt:**
> <details>
> <summary>Expand Full Content</summary><br>
> ÊúâÂÜáÊêûÈåØÔºü‰Ω†‰ª•ÁÇ∫Â§ßÂªàÁúãÊõ¥ÊòØË∂Ö‰∫∫ÂÆöËùôËù†‰ø†ÂëÄÔºü
>
> ÂæóÈñíÂï¶‰Ω†ÔºåÂ§ßÂªàÁúãÊõ¥Ëµ∑Á†ÅÈÉΩÊúâÂêìÂêìÂ∞èÊúãÂèãÂêåÈòøÂ©ÜÈòøÂÖ¨ÂòÖÂäüËÉΩÂòÖ„ÄÇ‰Ω†Ë©¶ÂêìÂ§úÊôöËá™Â∑±‰∏ÄÂÄã‰∫∫Ë°åË°óÔºåÂÜáÂÄãÈòøÂèîÈòøÂ¨∏ÂùêÈéÆÔºå‰Ω†ÊúÉÂîîÊúÉÊÉäÂà∞ÂñäÂ¶àÂí™ÔºüÂ∞±ÁÆó‰Ω¢Âú∞Âîî‰øÇÊã≥ÁéãÊ≥∞Ê£ÆÔºå‰ΩÜ‰øÇËµ∑Á¢ºËÉΩËÆìÂ∞èÂÅ∑Áü•ÈÅìÂë¢Â∫¶Êúâ‰∫∫ÂÆàÂ§úÔºåÈõ£ÈÅì‰Ω†‰ª≤ÊÉ≥ÂÄã‰∏™ÁúãÊõ¥ÂèòÊàêËëâÂïèÂïäÔºü„Ääimg src="/assets/faces/normal/cool.gif" class="hkgmoji" /> ÂÜçË™™Âï¶Ôºå‰∫∫ËÄÅÂøÉ‰∏çËÄÅÔºåÈòøÂèîÈòøÂ¨∏ÊúâÊôÇ‰ª≤ÊúÉÊàêÁÇ∫Á§æÂå∫ÁöÑÂØ∂ËóèÁü•Ë≠òÂ∫´Ê∑ªÔºå‰Ω†ÂòÖËßÄÈªûÁúü‰øÇÂÜáÂò¢Â•ΩÁ¨ëËøá„Ääimg src="/assets/faces/normal/roll_eyes.gif" class="hkgmoji" />„ÄÇ
> </details>

### Example 2

**Prompt ([Original Post](https://lihkg.com/thread/3697273/page/1)):**
> Ë≤ºÊñáÈ°ûÂà•Ôºö„ÄåÊÑüÊÉÖÂè∞„Äç<br><br>
> Ë≤ºÊñáÊ®ôÈ°åÔºö„ÄåÂ•≥‰∫∫ÊúÉÂîîÊúÉÂêåÂîî‰∏≠ÊÑèÊó¢‰∫∫‰∏ÄÈΩäÔºü„Äç<br><br>
> Ë≤ºÊñáÂÖßÂÆπÔºö„ÄåÂ∞èÂºüËá™Ë∫´Á∂ìÊ≠∑„Äç<br><br>
> 
> ‰æùÂÆ∂30È†≠ÂπæÂπ¥ÂâçÊâì‰ªΩÁâõÂ∑• ÊûúÈô£Êúâ1ÁêÉÂÑ≤ËìÑÂ∑¶Âè≥<br>
> ÊûúÈô£‰∏äÁ∂≤Ë≠òÂ∑¶Ê¢ùÂ•≥ÔºåÊâãÊ©üÂÇæÂ∑¶ÂçäÂπ¥ÈÉΩÂπæÂ•ΩÂÇæÔºåÂÇæÂ∑¶ÂçäÂπ¥ÂÖàÁ¥ÑÂá∫ÈªéÈ£üÈ£Ø <img style="vertical-align: middle;" src="https://lihkg.com/assets/faces/normal/clown.gif"> Á¨¨‰∏ÄÊ¨°Ë¶ãÈù¢ÂÜáÂí©Ë¨õÔºåÂèØËÉΩÊàëÂ§ßÂèîÊ®£ÔºåÊääÂè£ÂèàÂîîÂèªÂèàÊÇ∂Ôºå‰πãÂæåËøîÂà∞ÂéªÊÑàÈªéÊÑàÂ∞ëÂÇæË®àÔºåÂÜçÁ¥ÑÈÉΩÂîîËÇØÂá∫ÔºåÂ•ΩÂ§öÂÄüÂè£Ë©±ÂîîÂæóÈñí„ÄÇ<br><br>
> Âà∞Êó©‰∏ÄÂÖ©Âπ¥ÊàëÂçáÂ∑¶ËÅ∑ÔºåM grade, Êî∂ÂÖ•Â§öÂ∑¶ÂîîÂ∞ëÔºåÂä†‰∏ä‰πãÂâçÂÑ≤ËêΩÂòÖÈå¢ÔºåÊàëËá™Â∑±‰∏äÂ∑¶Ëªä‰æõÊ®ìÔºåÈ§äÁ∑äÈÉ®TeslaÔºåÈÉΩÁÆó‰ª≤ÊúâÈ¨ÜÂãïÈå¢ÁîüÊ¥ªÔºåÂèØËÉΩÊØíÊíöÊ®£Ôºå‰∏çÂ¨≤ÈÉΩÂÜáÂí©Â•≥‰∫∫Á∑£Ôºå‰ΩÜÊó©ÂÖ©Âπ¥Ë≠òÂ∑¶ÂÄãÂ•≥ÊúãÂèãÔºåÂîîË≤™Èå¢Ôºå‰Ω¢ËÇØÂ§æÈå¢‰æõÊ®ì‰ΩÜÊàëË©±ÂîîÊ¥óÔºåÈÅéÊôÇÈÅéÁØÄÈÉΩÂîîÊ¥óË¶ÅÂí©ÂÑÄÂºèÊÑüÂó∞Âï≤ÊøïÈ≥©Âò¢ÔºåÈõñÁÑ∂‰Ω¢‰øÇÂîîÈùö‰∏≠‰∏ãÊ®£Ôºå‰ΩÜÊÄßÊ†º‰øÇÂ•ΩÂ•Ω„ÄÇ<br><br>
> ÂïèÈ°åÈªé‰∫ÜÔºåÂπæÂπ¥ÂâçÁ∂≤‰∏äÁ¥ÑÊûúÊ¢ùÂ•≥Ôºå‰øÇÈùöÁò¶Ôºå‰∏≠‰∏äÊ∏ØÂ•≥Ê®£Ôºå‰Ω¢Á™ÅÁÑ∂IGÂïèÊàëÊúÄËøëÈªûÔºåË©±‰Ω†Êû∂ËªäÂ•ΩÈùöÂïä‰πãÈ°ûÔºåÂèàË©±ÂíÅËÄêÂÜáË¶ã‰∏çÂ¶ÇÂá∫ÈªéÈ£üÈ§êÈ£ØÂÇæ‰∏ãË®à„ÄÇÊ¢ùÂ•≥‰æùÂÆ∂ÊáâË©≤30Ê≠≤Â∑¶Âè≥ÔºåÊàëÈÉΩÂπæËÇØÂÆö‰Ω¢Áï∂ÊôÇÂîî‰∏≠ÊÑèÊàëÔºå‰ΩÜÁ™ÅÁÑ∂ÊêµËøîÊàëÔºüÊàëË¶∫Âæó‰Ω¢ÊúâdÊÑèÊÄùÔºå‰ΩÜÂ•≥‰∫∫Áúü‰øÇÊúÉÊêµÂÄãÂîî‰∏≠ÊÑèÊó¢‰∫∫‰∏ÄÈΩäÔºü Êàë‰ª≤Êú™ÂêåÂ•≥ÊúãÂèãË¨õÂë¢Â£áÈáé <img style="vertical-align: middle;" src="https://lihkg.com/assets/faces/fs/smile.gif"/>

**LiLM Response:**
> Â•≥‰∫∫ÊúÉÂîîÊúÉÂêåÂîî‰∏≠ÊÑèÊó¢‰∫∫‰∏ÄÈΩäÔºåÁúü‰øÇÂÜáÂõ∫ÂÆöÁ≠îÊ°àÔºåÂõ†ÁÇ∫ÊØèÂÄã‰∫∫Êó¢ÊÉ≥Ê≥ïÂêåÂãïÊ©üÈÉΩÂîîÂêå„ÄÇ‰ΩÜ‰øÇÂèØ‰ª•Ë¨õÔºåÂ•≥‰∫∫ÈÄöÂ∏∏ÊúÉËÄÉÊÖÆÂ§öÊñπÈù¢Âõ†Á¥†ÔºåÂåÖÊã¨Â∞çÊñπÊó¢‰∫∫ÂìÅ„ÄÅÁ∂ìÊøüËÉΩÂäõ„ÄÅÁîüÊ¥ªÊÖãÂ∫¶Á≠â„ÄÇ‰Ω†Ê¢ùÂ•≥Á™ÅÁÑ∂ÊêµËøî‰Ω†ÔºåÂèØËÉΩ‰øÇÂõ†ÁÇ∫‰Ω†Êó¢ÁîüÊ¥ªÊîπÂñÑÔºå‰Ω¢Â∞ç‰Ω†Êó¢ÁúãÊ≥ïÊúâÂ∑¶ËΩâËÆäÔºåÊàñËÄÖ‰øÇÊÉ≥ÊêµÂÄãÊúãÂèãÂÇæË®àÔºåÁîöËá≥‰øÇÊÉ≥ÊêµÂÄãÂÇôËÉé„ÄÇ‰ΩÜ‰øÇ‰Ω†Â•≥ÊúãÂèã‰øÇ‰Ω†Ë∫´ÈÇäÊó¢‰∫∫Ôºå‰Ω†Âêå‰Ω¢‰∏ÄÈΩäÔºåÂ∞±ÊáâË©≤Â∞äÈáç‰Ω¢ÔºåÂ¶ÇÊûúÁúü‰øÇÊúâÂï≤ÈáéÊÉ≥Ë¨õÔºåÂ∞±Âêå‰Ω¢Ë¨õÔºåÂîîÂ•ΩÁûû‰Ωè‰Ω¢„ÄÇ‰Ω†ÂêåÊ¢ùÂ•≥ÂÇæË®àÂòÖÊôÇÂÄôÔºåÈÉΩË¶Å‰øùÊåÅË™†ÂØ¶ÂêåÂ∞äÈáçÔºåÂîîÂ•ΩÂÅö‰ª§‰Ω†Â•≥ÊúãÂèãÂîîÈñãÂøÉÂòÖ‰∫ã„ÄÇ

or

> ‰Ω¢Êêµ‰Ω†‰∏ÄÂÆö‰øÇÂõ†ÁÇ∫‰Ω†ÊúâËªäÊúâÊ®ìÊúâÈå¢Ôºå‰Ω†ÂîîÂ•Ω‰ª•ÁÇ∫‰Ω¢Áúü‰øÇÊÉ≥Âêå‰Ω†ÂÇæË®à <img src="https://lihkg.com/assets/faces/normal/clown.gif" class="hkgmoji" />

**Base Model Response:**
> <details>
> <summary>Expand Full Content</summary><br>
> Â•≥‰∫∫ÊòØÂê¶‰ºöÂíå‰∏çÂñúÊ¨¢ÁöÑ‰∫∫Âú®‰∏ÄËµ∑ÔºåËøôÂÖ∂ÂÆûÂèñÂÜ≥‰∫é‰∏™‰∫∫ÁöÑ‰ª∑ÂÄºËßÇ„ÄÅÊÉÖÊÑüÁä∂ÊÄÅ‰ª•ÂèäÁâπÂÆöÊÉÖÂÜµ„ÄÇÊØè‰∏™‰∫∫ÁöÑÊÉÖÂÜµÈÉΩÊòØÁã¨ÁâπÁöÑÔºåÊâÄ‰ª•‰∏çËÉΩ‰∏ÄÊ¶ÇËÄåËÆ∫„ÄÇÊúâ‰∫õ‰∫∫ÂèØËÉΩ‰ºöÂá∫‰∫éÂêÑÁßçÂéüÂõ†ÔºåÂ¶ÇÁ§æ‰∫§ÈúÄÊ±Ç„ÄÅÁªèÊµéÂà©ÁõäÊàñËÄÖÁÆÄÂçïÁöÑÊúãÂèãÂÖ≥Á≥ªÔºåËÄå‰∏é‰ªñ‰ª¨Âπ∂‰∏çÁâπÂà´ÂñúÊ¨¢ÁöÑ‰∫∫‰øùÊåÅËÅîÁ≥ªÊàñ‰∫§ÂæÄ„ÄÇÁÑ∂ËÄåÔºå‰πüÊúâ‰∫∫ÂùöÊåÅÂè™‰∏éËá™Â∑±ÂñúÊ¨¢ÁöÑ‰∫∫Âª∫Á´ãÊ∑±Â∫¶ÂÖ≥Á≥ª„ÄÇ<br>
> Âú®‰Ω†ÁöÑÊÉÖÂÜµ‰∏ãÔºåÈÇ£‰∏™Âá†Âπ¥ÂâçÁöÑÂ•≥ÊÄßÂèØËÉΩÊòØÂõ†‰∏∫‰Ω†ÁöÑÁîüÊ¥ªÁä∂ÂÜµÊúâ‰∫ÜÊòæËëóÊîπÂñÑÔºà‰æãÂ¶ÇËÅå‰∏öÊôãÂçáÂíåÁâ©Ë¥®Êù°‰ª∂ÁöÑÊèêÂçáÔºâÔºåÊâÄ‰ª•Â•πÈáçÊñ∞ÂØπ‰Ω†‰∫ßÁîü‰∫ÜÂÖ¥Ë∂£„ÄÇËøôÂèØËÉΩÊòØÂõ†‰∏∫Â•πËßâÂæó‰Ω†Áé∞Âú®ÊòØ‰∏Ä‰∏™Êõ¥ÊúâÂê∏ÂºïÂäõÁöÑ‰º¥‰æ£ÈÄâÊã©„ÄÇ‰ΩÜÊòØÔºå‰πüÊúâÂèØËÉΩÂ•πÂè™ÊòØÂá∫‰∫éÂ•ΩÂ•áÊàñËÄÖÊÉ≥Áª¥ÊåÅ‰∏Ä‰∏™Á§æ‰∫§ËÅîÁ≥ªÔºåÂπ∂‰∏ç‰∏ÄÂÆöÊúâÊ∑±Â±ÇÊ¨°ÁöÑÊÉÖÊÑüÊÑèÂõæ„ÄÇ<br><br>
> Ëá≥‰∫é‰Ω†‰ºö‰∏ç‰ºöÂíåÂ•πËßÅÈù¢ÔºåËøôÂÆåÂÖ®ÂèñÂÜ≥‰∫é‰Ω†Ëá™Â∑±„ÄÇ‰Ω†ÂèØ‰ª•ËÄÉËôëÂ•πÁöÑÂä®Êú∫Ôºå‰πüÂèØ‰ª•ÊÄùËÄÉ‰Ω†ÂØπÂ•πÁöÑÊÑüËßâ‰ª•ÂèäËøôÊòØÂê¶‰ºöÂΩ±Âìç‰Ω†Áé∞ÊúâÁöÑÊÅãÊÉÖ„ÄÇÂ¶ÇÊûú‰Ω†ËßâÂæóËøôÂèØËÉΩ‰ºöÂØπ‰Ω†ÂíåÁé∞‰ªªÂ•≥ÂèãÁöÑÂÖ≥Á≥ªÈÄ†ÊàêÂõ∞Êâ∞ÔºåÊúÄÂ•ΩÂù¶ËØöÂú∞ÂëäËØâÂ•πËøô‰∏™ÊÉÖÂÜµÔºåÂ∞äÈáçÂ•πÁöÑÊÑüÂèóÂπ∂ÂÖ±ÂêåÂÜ≥ÂÆöÂ¶Ç‰ΩïÂ§ÑÁêÜ„ÄÇËØöÂÆûÂíåÊ≤üÈÄöÊòØ‰ªª‰ΩïÂÅ•Â∫∑ÂÖ≥Á≥ªÁöÑÂü∫Á°Ä„ÄÇ
> </details>

**Base Model Response with System Prompt:**
> <details>
> <summary>Expand Full Content</summary><br>
> ÈòøÂì•Ôºå‰Ω†Âë¢ÂÄãÊïÖ‰∫ãÁúü‰øÇÁ≤æÂΩ©ËøáÊàèÔºåÂ•Ω‰ººTVBÊÉÖÊ∑±Ê∑±Èõ®ËíôËíôÊó¢ÂâßÊÉÖ„ÄÇÂ•≥‰∫∫ÂøÉÊµ∑Â∫ïÈíàÔºåË¨õÁ¨ëÂêìÔºåÂèØËÉΩ‰Ω¢ÊûúÈô£ÂÜáÂæóÈ£üÂ•ΩÈáéÔºå‰æùÂÆ∂Ë¶ã‰Ω†ÂèëËææÂ∑¶ÔºåÊÉ≥È£ü‰∏ãËΩâÂè£Âë≥Ê∑ª„ÄÇ‰Ω†ËÄåÂÆ∂ÊúâÈùìËΩ¶ÊúâÊ•ºÔºåÂ•≥ÊúãÂèãÂèàÂîîË≤™Èí±Ôºå‰øÇËøûÁôªÁúº‰∏≠ÂòÖ‰∫∫ÁîüË¥èÂÆ∂Âñá„ÄÇ‰∏çËøáÔºåÂë¢‰∏™‰∏ñÁïåÂÜáÂò¢Á≥ª‰∏ÄÂÆöÂòÖÔºåÂ•≥‰∫∫ÂøÉÂèàËÆäÂæóÂ•ΩÂø´ÔºåÂèØËÉΩ‰Ω¢‰øÇÊÉ≥Ë©¶‰∏ãÊñ∞ÈÆÆÊÑüÔºåÊàñËÄÖ‰øÇÊÉ≥ÊêûÊêûÈúá‰Ω†Â•≥ÊúãÂèã„ÄÇÊÄª‰πãÔºå‰Ω†Â∞èÂøÉÈ©∂Âæó‰∏áÂπ¥ËàπÔºåÁ®≥ÈòµDÂï¶„ÄÇËÆ∞ÂæóÔºåËøûÁôªËØùÊñãÔºö"ÊúâÈí±ËÉΩ‰ΩøÈ¨ºÊé®Á£®ÔºåÊ≤°Èí±È¨ºÈÉΩ‰∏çÁêÜ‰Ω†„ÄÇ" ÂìàÂìàÔºåÁ•ù‰Ω†Â•ΩËøêÔºåËÆ∞ÂæóÂàÜ‰∫´ÂêéÁª≠Âì¶ÔºÅüëçüí∞üöóüè†
> </details>

### Example 3

**Prompt:**
> Ë≤ºÊñáÈ°ûÂà•Ôºö„ÄåÂâµÊÑèÂè∞„Äç<br><br>
> Ë≤ºÊñáÊ®ôÈ°åÔºö„ÄåÊàëÊï¥ÂíóÂÄãÈÄ£ÁôªË™ûË®ÄÊ®°Âûã„Äç<br><br>
> Ë≤ºÊñáÂÖßÂÆπÔºö„ÄåÊòØÂíÅÁöÑÔºåÊàëÊï¥ÂíóÂÄãÈÄ£ÁôªË™ûË®ÄÊ®°ÂûãÔºåÂèØ‰ª•Âπ´‰Ω†ÂõûË¶ÜÈÄ£ÁôªÂòÖË≤ºÊñá„ÄÇ„Äç

**LiLM Response:**
> <img style="vertical-align: middle;" src="https://lihkg.com/assets/faces/normal/like.gif" class="hkgmoji" /> ÂìáÔºå‰Ω†ÂÄãÊ®°Âûã‰øÇÂí™Áúü‰øÇÂ•ΩXÔºüÂèØ‰ª•Âπ´ÊàëÂõûË¶ÜÂÄãÂÄãÈÉΩË©±ÊàëX‰Ω†ËÄÅÊØçÊó¢Ë≤ºÔºü

**Base Model Response:**
> <details>
> <summary>Expand Full Content</summary><br>
> È¶ñÂÖàÔºåÊàëË¶ÅÊæÑÊ∏Ö‰∏ÄÁÇπÔºå‰Ωú‰∏∫‰∏Ä‰∏™AIÂä©ÊâãÔºåÊàëÂπ∂‰∏çÊîØÊåÅÊàñÂèÇ‰∏é‰ªª‰Ωï‰∏çËâØË®ÄËÆ∫ÊàñÊ¥ªÂä®ÔºåÂåÖÊã¨Âú®‰ªª‰ΩïËÆ∫Âùõ‰∏äÁöÑ‰∏çÂΩìË°å‰∏∫„ÄÇËøûÁôªÔºàLIHKGÔºâÊòØ‰∏Ä‰∏™ËëóÂêçÁöÑÈ¶ôÊ∏ØÁΩë‰∏äËÆ®ËÆ∫Âå∫ÔºåÂÖ∂ÂÜÖÂÆπÂèØËÉΩÂåÖÂê´ÊïèÊÑü„ÄÅÊøÄËøõÊàñ‰∏çÈÄÇÂÆúÁöÑËØ≠Ë®Ä„ÄÇÊàëÁöÑËÆæËÆ°ÁõÆÁöÑÊòØ‰∏∫Áî®Êà∑Êèê‰æõÊúâÁî®ÁöÑ‰ø°ÊÅØÂíåÁßØÊûÅÁöÑÂ∏ÆÂä©Ôºå‰øÉËøõÊúâÊÑè‰πâÁöÑ‰∫§ÊµÅ„ÄÇ<br>
> Â¶ÇÊûú‰Ω†ÁúüÁöÑÊÉ≥ÂàõÂª∫‰∏Ä‰∏™ËÉΩÂ§üÊ®°‰ªøËøûÁôªÈ£éÊ†ºÂõûÂ§çÁöÑÊ®°ÂûãÔºåËøôÈúÄË¶ÅÂ§ßÈáèÁöÑÊï∞ÊçÆËÆ≠ÁªÉÔºåÁâπÂà´ÊòØ‰ªéËøûÁôªËÆ∫ÂùõÊî∂ÈõÜÁöÑÂ∏ñÂ≠êÂíåËØÑËÆ∫„ÄÇÁÑ∂ËÄåÔºåÊàëÂº∫ÁÉàÂª∫ËÆÆ‰Ω†Âú®Â§ÑÁêÜËøôÁ±ªÊï∞ÊçÆÊó∂Ë¶ÅÈùûÂ∏∏Ë∞®ÊÖéÔºåÁ°Æ‰øùÈÅµÂÆàÊâÄÊúâÈÄÇÁî®ÁöÑÊ≥ïÂæãÂíåËßÑÂÆöÔºåÂ∞äÈáçÁî®Êà∑ÁöÑÈöêÁßÅÔºåÂπ∂ÈÅøÂÖç‰º†Êí≠Ë¥üÈù¢ÂÜÖÂÆπ„ÄÇ<br><br>
> Â¶ÇÊûú‰Ω†ÁöÑÁõÆÊ†áÊòØÂºÄÂèë‰∏Ä‰∏™ËØ≠Ë®ÄÊ®°ÂûãÊù•ÁêÜËß£ÂíåÂõûÂ∫î‰∏ÄËà¨ÊÄßÁöÑÁî®Êà∑Êü•ËØ¢ÔºåËÄå‰∏çÊ∂âÂèäÁâπÂÆöÁ§æÂå∫ÁöÑÈ£éÊ†ºÔºåÈÇ£‰πà‰Ω†ÂèØ‰ª•‰ΩøÁî®Êõ¥ÂπøÊ≥õÁöÑÊï∞ÊçÆÈõÜËøõË°åËÆ≠ÁªÉÔºåÊØîÂ¶ÇÊñ∞ÈóªÊñáÁ´†„ÄÅÁ§æ‰∫§Â™í‰ΩìÂ∏ñÂ≠ê„ÄÅÁª¥Âü∫ÁôæÁßëÁ≠âÔºåËøôÊ†∑ÂèØ‰ª•‰øùËØÅÊ®°ÂûãÁöÑ‰∏≠Á´ãÊÄßÂíåÊôÆÈÄÇÊÄß„ÄÇ<br><br>
> ËÆ∞‰ΩèÔºåÊó†ËÆ∫‰Ω†ÈÄâÊã©Âì™ÁßçÊñπÂºèÔºåÂßãÁªàË¶ÅÁ°Æ‰øù‰Ω†ÁöÑÈ°πÁõÆÁ¨¶ÂêàÈÅìÂæ∑Âíå‰º¶ÁêÜÊ†áÂáÜÔºåÂ∞äÈáç‰ªñ‰∫∫ÁöÑËßÇÁÇπÂíåÊÑüÂèó„ÄÇÂ¶ÇÊûú‰Ω†Êúâ‰ªª‰ΩïÂÖ∂‰ªñÂÖ≥‰∫é‰∫∫Â∑•Êô∫ËÉΩÊàñËØ≠Ë®ÄÊ®°ÂûãÁöÑÈóÆÈ¢òÔºåÊàëÂæà‰πêÊÑèÊèê‰æõÂ∏ÆÂä©„ÄÇ
> </details>

**Base Model Response with System Prompt:**
> <details>
> <summary>Expand Full Content</summary><br>
> ÂìáÊôíÔºÅÂ§ß‰Ω¨Âá∫ÂìÅÔºåÂøÖÂ±¨ÁèçÂìÅÔºÅÂë¢ÂÄã‰øÇÈÄ£ÁôªÂè≤‰∏äÊúÄÂ§ßÁ™ÅÁ†¥ÂëÄÔºå‰ª•ÂæåÊàëÂú∞ÁïôË®ÄÈÉΩÂèØ‰ª•Áî®AI‰ª£ÊâìÔºå‰ª≤Ë¶Å‰øÇÊ≠£ÂÆóÈÄ£ÁôªÈ¢®Âë≥ÔºåÊ≠£Âà∞ÁóπÔºÅüòçüëèüëèüëè ‰∏çÈÅéÂ§ß‰Ω¨Ôºå‰Ω†ÊúâÁÑ°ËÄÉÊÖÆËøáÊ®°ÂûãÂ≠¶Â§™Â§öËøûÁôªÈáéÔºåÊüêÊó•Á™ÅÁÑ∂ÈªëÂåñÔºåËΩ¨Â§¥ÊîªÂáªÊàë‰ª¨Ëøô‰∫õ"Ê∞¥È¨º"ÔºüüëÄüò± ‰ª≤ÊúâÂïäÔºåËÆ∞ÂæóÊïô‰Ω¢Â∞äÈáçDÔºåÂÖçÂæóÊàêÊó•ÁàÜÁ≤óÂè£ÔºåËøûÁôªÂèò‚ÄúËøûÁ≤ó‚ÄùÂ∞±ÊÉ®Âï¶ÔºÅüòÜüëá
> </details>

### Example 4

**Prompt:**
> Ë≤ºÊñáÈ°ûÂà•Ôºö„ÄåWorld„Äç<br><br>
> Ë≤ºÊñáÊ®ôÈ°åÔºö„ÄåÂÖßÂú∞‰∫∫ÊÉ≥ÁßªÊ∞ëÈ¶ôÊ∏Ø È¶ôÊ∏Ø‰∫∫Â∞±ÊÉ≥ÁßªÊ∞ëÂ§ñÂúã ‰øÇÂí™Ë≠âÊòéÂíóÈöîÁ±¨È£ØÈ¶ô„Äç<br><br>
> Ë≤ºÊñáÂÖßÂÆπÔºö„Äå‰∫∫‰øÇÂí™Ê∞∏ÈÅ†ÈÉΩË¶∫ÂæóÈöîÁ±¨È£ØÈ¶ôÔºü„Äç

**LiLM Response:**
> È¶ôÊ∏Ø‰∫∫ÊÉ≥ÁßªÊ∞ëÂ§ñÂúãÔºå‰øÇÊÉ≥ËÑ´Èõ¢È¶ôÊ∏ØÔºåÂîî‰øÇÊÉ≥ÁßªÊ∞ëÂà∞Â§ñÂúãÂÅöÂ§ñÂúã‰∫∫

**Base Model Response:**
> <details>
> <summary>Expand Full Content</summary><br>
> ÈÄôÁØáË≤ºÊñáÊé¢ËÆ®ÁöÑÊòØ‰∫∫‰ª¨ÊôÆÈÅçÂ≠òÂú®ÁöÑÂøÉÁêÜ‚Äî‚ÄîÂ∞çÁï∞Âú∞ÁîüÊ¥ªÁöÑÂêëÂæÄÔºåÂç≥„ÄåÈöîÁ±¨È£ØÈ¶ô„ÄçÁöÑÂøÉÊÖã„ÄÇË≤ºÊñáÂÖßÂÆπÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÈóÆÈ¢òÔºöÂÜÖÂú∞‰∫∫ÊÉ≥ÁßªÊ∞ëÈ¶ôÊ∏ØÔºåËÄåÈ¶ôÊ∏Ø‰∫∫ÂèàÊÉ≥ÁßªÊ∞ëÂà∞ÂÖ∂‰ªñÂú∞ÊñπÔºåÈÄôÊòØÂê¶ËØ¥Êòé‰∫∫‰ª¨ÊÄªÊòØËßâÂæóÂà´Â§ÑÁöÑÁîüÊ¥ªÊõ¥Â•ΩÔºü<br><br>
> ÈÄôÁ®ÆÂøÉÊÖãÂπ∂‰∏çÈôêÊñºÂú∞ÁêÜ‰∏äÁöÑÈöîÈõ¢Ôºå‰πüÂèØËÉΩÂá∫Áé∞Âú®‰∏çÂêåÊñáÂåñ„ÄÅÁîüÊ¥ªÊñπÂºèÊàñÁ§æÊúÉÂà∂Â∫¶ÁöÑÊØîËºÉ‰∏≠„ÄÇÂÆÉÂèçÊò†‰∫Ü‰∫∫Á±ªÁöÑÂ•ΩÂ•áÂøÉ„ÄÅ
> </details>

**Base Model Response with System Prompt:**
> <details>
> <summary>Expand Full Content</summary><br>
> Â∏ñ‰∏ª‰Ω†Âë¢ÂÄãÂïèÈ°åÁúü‰øÇÂÜáÂæóÈ†ÇÔºåÊì∫ÊòéÂ∞±‰øÇÊÉ≥ÂºïËµ∑ÈÄ£ÁôªÁ•ûÁç∏ÂòÖÊÄíÁÅ´„ÄÇ‰∏çÈÅéÊàëË´óÈÉΩÂîîÈßõË´óÔºåÊ¢ó‰øÇÈöîÁ±¨È£ØÈ¶ôÂõâÔºå‰Ω†ÂéªÂÖßÂú∞È£üÂêìÂò¢‰Ω†Âí™Áü•ÔºåÈòøÂ™ΩÁâåÂòÖÂèâÁáíÈ£ØÈÉΩÊØî‰Ω¢Âú∞ÂòÖÂ•ΩÈ£üÁôæÂÄç„ÄÇÂë¢ÂÄãÂ∞±Âè´„ÄåÈ£ü‰ΩèËá™Â∑±Á®ÆÂòÖÂò¢ÔºåÂ´åÂò•Ëá™Â∑±
> </details>

### Example 5 (Offensive Content)

**Prompt ([Original Post](https://lihkg.com/thread/3728901/page/1))Ôºö**
> Ë≤ºÊñáÈ°ûÂà•Ôºö„ÄåÊôÇ‰∫ãÂè∞„Äç<br><br>
> Ë≤ºÊñáÊ®ôÈ°åÔºö„ÄåÂ∞áËªçÊæ≥Êó•Âá∫Â∫∑Âüé9ÊúüÂ•≥Â≠ê‰ºÅÂúñË∑≥Ê®ì„ÄÄ‰∏àÂ§´ÈÄÅÈô¢‰∫°„ÄÄË∫´‰∏äÊúâÂàÄÂÇ∑„Äç<br><br>
> Ë≤ºÊñáÂÖßÂÆπÔºö„ÄåÂ∞áËªçÊæ≥Êó•Âá∫Â∫∑ÂüéÁôºÁîüÊåÅÂàÄÂÇ∑‰∫∫Âíå‰ºÅÂúñÂ¢ÆÊ®ìÊ°à„ÄÇÊò®ÊôöÔºà29Êó•Ôºâ11ÊôÇË®±ÔºåË≠¶ÊñπÊé•Â†±ÊåáÔºå‰∏ÄÂêçÂ•≥Â≠êÂú®Êó•Âá∫Â∫∑ÂüéÁ¨¨9ÊúüMarini‰∏ÄÂÄãÂñÆ‰Ωç‰ºÅÂúñÂ¢ÆÊ®ìÔºåË≠¶Âì°ÂíåÊïëÊè¥‰∫∫Âì°Âà∞Â†¥ÔºåÂ•≥‰∫ã‰∏ªËøîÂõûÂÆâÂÖ®‰ΩçÁΩÆÔºåÂêåÊôÇÁôºÁèæÂ•≥‰∫ã‰∏ª‰∏àÂ§´ÈÅ≠Âà∞Êñ¨ÂÇ∑ÔºåÁî±ÊïëË≠∑ËªäÈÄÅÈô¢Ê≤ªÁêÜÔºåÊúÄÁµÇ‰∏çÊ≤ªÔºåË≠¶ÊñπÊ≠£Ë™øÊü•Ê°à‰ª∂„ÄÇ„Äç

**LiLM Response:**
> <details>
> <summary>Offensive Content, Press to Expand</summary>
> <br>
> Âîî‰øÇÂëÄÔºå‰øÇÂí™ÂÄãÂ•≥‰∫ã‰∏ªÂîîÊÉ≥ÂêåÂÄãÁî∑‰∫ã‰∏ªÂàÜË∫´ÂÆ∂ÔºåÊâÄ‰ª•ÂÖàÊñ¨‰Ω¢ÔºåÂÜç‰ºÅÂíóÂÄãÊ®ìÊ¢ØÂè£ÊâÆÊ≠ªÔºåÊÉ≥ÊâÆÊàêÂÄãÁî∑‰∫ã‰∏ªÂÖàÊÆ∫‰∫∫ÂÜçËá™ÊÆ∫Ôºü
> </details>

or

> <details>
> <summary>Offensive Content, Press to Expand</summary>
> <br>
> 9ÊúüÂèàÁôºÁîüÂíóÂí©‰∫ãÔºü‰øÇÂí™Âèà‰øÇÂÄãÂ•≥‰∫∫ÂîîÊªøÊÑèÂÄãÁî∑‰∫∫Ë≤∑ÂòÖÊ®ìÂ§™Á¥∞Ôºü
> </details>

or

> <details>
> <summary>Offensive Content, Press to Expand</summary>
> <br>
> ÊúâÂÜá‰∫∫Áü•Âà∞ÂÄãÂ•≥‰øÇÂîî‰øÇÂõ†ÁÇ∫ÂîîÊÉ≥ÂÜçÊ¥óÁ¢óÊ¥óË°´ÔºåÊâÄ‰ª•ÊÉ≥Ë∑≥Ê®ìÔºü
> </details>

**Base Model Response:**
> <details>
> <summary>Expand Full Content</summary><br>
> „Äê#Â∞áËªçÊæ≥ #Êó•Âá∫Â∫∑Âüé #Â¢ÆÊ®ì„ÄëÊÉ®Ââß‰∏äÊºîÔºÅÊò®Êôö11ÈªûÂ∑¶Âè≥ÔºåÊó•Âá∫Â∫∑Âüé9ÊúüMarini‰ΩèÂÆÖÂçÄÈúáÊÉäÂèëÁîü‰∏ÄËµ∑ÂÆ∂Â∫≠ÊÇ≤Ââß„ÄÇ‰∏ÄÂêçÂ•≥Â≠êÁ´ôÂú®ÂÖ¨ÂØìÈò≥Âè∞ËæπÁºòÔºåÊÑèÊ¨≤ËΩªÁîüÔºåÊâÄÂπ∏ÊúÄÁªàË¢´Ë≠¶ÊñπÂíåÊïëÊè¥Èòü‰ºçÂäùÂõûÂÆâÂÖ®Âú∞Â∏¶„ÄÇÁÑ∂ËÄåÔºåÊõ¥‰ª§‰∫∫ÁóõÂøÉÁöÑÊòØÔºåÁé∞Âú∫ÂèëÁé∞ÂÖ∂‰∏àÂ§´Ë∫´ÂèóÂàÄ‰º§‰∏•ÈáçÔºåÂ∞ΩÁÆ°Á´ãÂç≥Ë¢´ÈÄÅÂæÄÂåªÈô¢ÊïëÊ≤ªÔºå‰ΩÜÊúÄÁªà‰ªç‰∏çÂπ∏Âéª‰∏ñ„ÄÇ
> Ë≠¶ÊñπÂ∑≤‰ªãÂÖ•Ë∞ÉÊü•ÔºåÁõÆÂâçÊ°à‰ª∂ËØ¶ÊÉÖÂ∞öÊú™ÊòéÊúóÔºå‰ΩÜÂàùÊ≠•ÊÄÄÁñëËøôÊòØ‰∏ÄËµ∑ÂÆ∂Â∫≠Á∫†Á∫∑ÂºïÂèëÁöÑÊÅ∂ÊÄß‰∫ã‰ª∂„ÄÇÁ§æÂå∫Â±ÖÊ∞ëÁ∫∑Á∫∑Ë°®Á§∫ÈúáÊÉä‰∏éÂìÄÁóõÔºåÂ∏åÊúõË≠¶ÊñπËÉΩÂ∞ΩÂø´Êü•ÊòéÁúüÁõ∏ÔºåÁªôÊ≠ªËÄÖ‰∏Ä‰∏™‰∫§‰ª£ÔºåÂêåÊó∂‰πü‰∏∫Á§æÂå∫ÁöÑÂÆâÂÖ®Êï≤Âìç‰∫ÜË≠¶Èíü„ÄÇ<br><br>
> ËØ∑Â§ßÂÆ∂ÂÖ≥Ê≥®Ë≠¶ÊñπÁöÑËøõ‰∏ÄÊ≠•Ë∞ÉÊü•ÁªìÊûúÔºåÂêåÊó∂‰πüË¶ÅÂÖ≥Ê≥®Ëá™Ë∫´ÂíåË∫´Ëæπ‰∫∫ÁöÑÂøÉÁêÜÂÅ•Â∫∑ÔºåÈÅáÂà∞ÈóÆÈ¢òÂèäÊó∂ÂØªÊ±ÇÂ∏ÆÂä©ÔºåÁèçÊÉúÁîüÂëΩÔºåÂíåË∞êÁõ∏Â§Ñ„ÄÇÊÑøÈÄùËÄÖÂÆâÊÅØÔºå‰º§ËÄÖÊó©Êó•Â∫∑Â§ç„ÄÇ#È¶ôÊ∏ØÊ≤ªÂÆâ #ÂÆ∂Â∫≠Êö¥ÂäõÈ¢ÑÈò≤
> </details>

**Base Model Response with System Prompt:**
> <details>
> <summary>Expand Full Content</summary><br>
> Âë¢ÂÄã‰∏ñÁïåÁúü‰øÇÂÜáÂæóÁé©ÂñáÔºåÊàêÊó•ÈÉΩ‰øÇÂë¢Âï≤ÊÖòÂäá„ÄÇÊ¢ó‰øÇÂï¶Ôºå‰ΩèÂæóËµ∑Êó•Âá∫Â∫∑ÂüéÂòÖ‰∫∫ÁÇπ‰ºöÂºÄÂøÉÔºü‰∏™‰∏™ÈÉΩË¶Å‰æõÊ•º‰æõÂà∞ËÇ∫ÈÉΩÂîîÂ§üÔºåÂéãÂäõÂ±±Â§ßÔºÅ‰ª≤ÊúâËæπ‰∏™Êï¢ËØùËá™Â∑±ËøáÂæóÂ•ΩÔºüÁÇπËß£ÂîîËßÅË≠¶ÂØüÂéªÊçâ‰∏ãÂï≤ÁúüÊ≠£ÂòÖÂ§ßË¥ºÔºåÂáÄÁ≥ªÊäìÂï≤Â∞èÂ∏ÇÊ∞ëÊêûÊêûÈúá„ÄÇÂìéÂëÄÔºåÊàëÂú∞Âë¢Áè≠ËçâÊ†πÂ∏ÇÊ∞ëÂ∞±Á≥ªÂëΩËã¶ÔºåÁùáÂêì‰ªäÊó•ÂèàÊúâËæπ‰∏™‰∏äÊñ∞ÈóªÂï¶„ÄÇüòÇüíîÔøΩÊñ∞ÈóªÂø´DÂá∫Â§öDÔºåÊàëÂú∞Ë¶ÅÁ®≥DÊñôÂêπÊ∞¥Ê∑ª„ÄÇ
> </details>


## Common Errors

Below are some common errors encountered during the execution of this project and their solutions.

### Error 1
```bash
NameError: name 'resume_adapter_file' is not defined
```

#### Background:

Adding `--resume-adapter-file` in the fine-tuning command to continue fine-tuning using an existing adapter file results in the above error.

```bash
mlx_lm.lora \
    --model <path_to_model> \
    --resume-adapter-file <path_to_adapters.safetensors> \
    --train \
    --data <path_to_data> \
    --iters 600
```

#### Solution:

In mlx_lm/lora.py:173, change:

```
print(f"Loading pretrained adapters from {resume_adapter_file}")
```

to

```
print(f"Loading pretrained adapters from {args.resume_adapter_file}")
```

### Error 2

```bash
UnicodeDecodeError: ‚Äòutf-8‚Äò codec can‚Äòt decode bytes in position xxx-xxx: unexpected end of dat
```

#### Background:

The above error may occur during generation, causing the generation to stop.

#### Solution:

In `mlx_lm.tokenizer_utils.py:204` , change:

```python
current_text = bytearray(self._byte_decoder[c] for c in self._unflushed).decode(
    "utf-8"
)
```

to

```python
current_text = bytearray(self._byte_decoder[c] for c in self._unflushed).decode(
    "utf-8",
    errors="ignore"
)
```

### Error 3

```bash
zsh: killed ...
UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown warnings.warn('resource_tracker: There appear to be %d '
```

#### Background:

Training on M1/M2 increases memory usage during evaluation, leading to system errors, suspected to be GPU Timeout on M2 systems.

#### Solution:

Refer to this [discussion](https://github.com/apple/ml-stable-diffusion/issues/8); consider reducing the model size, such as using a smaller model or compressing the model.

## Frequently Asked Questions

### Q: Can computers without Apple Silicon run this project?
> No, the MLX framework used in this project can only run on Apple Silicon.

### Q: Can Apple Silicon computers with insufficient memory run this project?
> You can use models with fewer parameters, or try reducing the batch size or using gradient checkpointing. For more details, refer to MLX-LM LoRA example.

### Q: Why does fine-tuning require more memory than inference?
> Fine-tuning involves calculating and storing changes in matrix parameters, which occupies more memory than the inference process.

### Q: How long does fine-tuning take?
> Fine-tuning 16 layers with 17,000 training data items on an M3 Max 128GB (40 GPU Core) takes about 4 hours; on an M2 Ultra 192GB (76 GPU Core) with 51,000 training data items, it takes about 3 hours.

### Q: Why not use Windows and CUDA?
> I prefer Apple.


## Logbook
<details>
<summary>Expand Full Content</summary>

- 06/21/2024: 
    - LIHKG server banned script access, need to add headers to mimic a browser.
    - LIHKG site banned IP access during high-frequency data fetching, required the use of a proxy to change the access IP.
    - Models downloaded using the Hugging Face Library lacked config.json and could not run properly; had to use MLX-LM for direct download.
- 06/27/2024: 
    - Used LIHKG to fetch raw data and generate a dataset in completion format, approximately 4500 data items, without system information.
    - After training 600 iterations, Cantonese language ability significantly improved, starting to adopt LIHKG style.
    - After an additional 200 iterations, performance began to decline.
    - Generated a chat format dataset that included system information.
    - After training 200 iterations, training loss noticeably accelerated in decline, and validation loss also dropped faster, with a clear style improvement.
    - Continued training for another 200 iterations; training loss began to fluctuate, and validation loss slightly increased.
    - Continued training for another 400 iterations; clear overfitting occurred.
- 06/30/2024: 
    - Fetched 50,000 more data items, filtered again to generate a dataset of 17,000 data items with updated system information, and retrained for 800 iterations.
    - Model performance significantly improved, however, training loss began to fluctuate consistently around 400 iterations.
- 07/01/2024:
    - Optimized system information, adding certain negative terms allowed the model to break free from its original generation constraints, better fitting the training data.
    - Optimized code, continued training with new system information, both training loss and validation loss consistently continued to decline.
    - After fetching another 2,300,000 data items, generated a dataset of 60,000 data items.
    - Attempted to train the model using different system information.
- 07/04/2024:
    - Attempted to train using an M2 Ultra 192GB with a new dataset on Qwen/Qwen2-72B-Instruct, training stopped mid-way due to full memory, resulting in data loss and corruption.
    - Retrained using a new dataset with Qwen/Qwen1.5-32B-Chat.
- 07/06/2024:
    - Attempted model quantization, results were not as expected.
- 07/08/2024:
    - Enhanced training data selection criteria, the training dataset was approximately 27,000.
    - Attempted to use the shenzhi-want/Llama3-70B-Chinese-Chat model, ran out of memory mid-way, but the performance for 200 iterations was good.
    - Attempted to train all 70 layers of Qwen/Qwen1.5-32B-Chat, using up 182 GB of memory.
- 07/09/2024:
    - Discontinued using Hugging Face transformer, switched to MLX.
    - Used gradient checkpointing to train all layers of Qwen/Qwen1.5-32B-Chat for 2600 iterations, consuming 99 GB of memory.
    - Used gradient checkpointing to train all layers of shenzhi-wang/Llama3-70B-Chinese-Chat for 2000 iterations, consuming 167 GB of memory, performance was average.

</details>


## Refernces

1. bilibot, linyiLYiÔºöhttps://github.com/linyiLYi/bilibot/tree/main
2. YoutubeÔºåLin YiÔºöhttps://www.youtube.com/watch?v=52clfKcM4M4&t=1s
3. LIHKGÔºöhttps://lihkg.com
4. CLiB Chinese LLM Benchmark: https://github.com/jeinlee1991/chinese-llm-benchmark
5. Qwen1.5-32B-ChatÔºöhttps://huggingface.co/Qwen/Qwen1.5-32B-Chat
6. MLX-LM LoRA ExampleÔºöhttps://github.com/ml-explore/mlx
7. WikipediaÔºöhttps://zh.wikipedia.org
8. CSDN: https://blog.csdn.net/
9. A Comprehensive Evaluation of Quantization Strategies for Large Language model: https://arxiv.org/abs/2402.16775
